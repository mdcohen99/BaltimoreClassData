original_colnames <- colnames(table_df); if(is.null(original_colnames)) original_colnames <- paste0("X", 1:ncol(table_df)); clean_colnames <- tolower(gsub("[^[:alnum:]]", "", original_colnames)); clean_colnames[clean_colnames == ""] <- paste0("empty", seq_along(clean_colnames[clean_colnames == ""])); colnames(table_df) <- make.unique(clean_colnames)
jurisdiction_col_idx <- 1; claims_col_idx <- NULL; possible_claims_cols <- c("initialclaims", "claims", "number", "count", "total"); claims_col_match <- intersect(possible_claims_cols, colnames(table_df))
if (length(claims_col_match) > 0) { claims_col_idx <- which(colnames(table_df) == claims_col_match[1]) } else if (ncol(table_df) >= 2) { claims_col_idx <- 2 }
if (is.null(claims_col_idx)) { return(NULL) }
for (county in counties) {
row_matches <- which(grepl(county, table_df[[jurisdiction_col_idx]], ignore.case = TRUE))
if (length(row_matches) > 0) {
exact_match_pattern <- paste0("^\\s*", gsub("'", "['`]?s?", county), "\\s*$"); exact_row_idx <- which(grepl(exact_match_pattern, table_df[[jurisdiction_col_idx]], ignore.case = TRUE)); row_idx <- if(length(exact_row_idx) > 0) exact_row_idx[1] else row_matches[1]
claims_value_raw <- table_df[row_idx, claims_col_idx]
claims_value <- tryCatch({ cleaned_str <- gsub("[^0-9.]", "", as.character(claims_value_raw)); if (nchar(cleaned_str) == 0 || cleaned_str == ".") NA_real_ else as.numeric(cleaned_str) }, warning = function(w) NA_real_, error = function(e) NA_real_)
if (!is.na(claims_value)) { results <- rbind(results, data.frame(jurisdiction = county, initial_claims = claims_value, stringsAsFactors = FALSE)) }
}
}
if (nrow(results) > 0) { return(results) } else { return(NULL) }
}
# --- Main RSelenium Debugging Function ---
debug_md_ui_claims_selenium <- function() {
driver <- NULL
remDr <- NULL
all_data <- list() # Keep this for potential success
# --- 1. Start RSelenium Driver ---
tryCatch({
free_port <- netstat::free_port()
cat("Attempting to start Selenium driver on port", free_port, "\n")
driver <- rsDriver(browser = "chrome", port = free_port, verbose = FALSE, chromever = "latest")
remDr <- driver$client
cat("Selenium driver started successfully.\n")
# Set longer implicit wait - Selenium will wait up to 10s for elements to appear
remDr$setTimeout(type = "implicit", milliseconds = 10000)
}, error = function(e) {
cat("Error starting Selenium driver: ", e$message, "\n")
return(data.frame()) # Return empty frame if driver fails
})
if (is.null(remDr)) return(data.frame())
# --- 2. Debug Specific URL ---
tryCatch({
cat("\n----------------------------\n")
cat("Processing URL:", url_to_debug, "\n")
# Navigate
cat("Navigating to URL...\n")
remDr$navigate(url_to_debug)
Sys.sleep(2) # Short sleep after initial nav command returns
cat("Navigation command sent. Current URL:", remDr$getCurrentUrl(), "\n")
cat("Page Title:", remDr$getTitle(), "\n")
# --- !!! PASTE YOUR XPATHS HERE !!! ---
# Replace these example XPaths with the ones you copied from the browser inspector
# Example: xpath_header <- "/html/body/div[2]/div/main/article/div/p[5]/strong"
# Example: xpath_table <- "/html/body/div[2]/div/main/article/div/table[1]"
xpath_header <- "/html/body/div[1]/div[3]/div/div/div[1]/article/div/p[5]/strong" #<-- REPLACE THIS
xpath_table <- "/html/body/div[1]/div[3]/div/div/div[1]/article/div/table[1]" #<-- REPLACE THIS
# --- !!! END OF XPATH SECTION !!! ---
# Check for Iframes (optional but good practice)
iframes <- remDr$findElements(using = 'tag name', 'iframe')
if (length(iframes) > 0) {
cat("WARNING: Found", length(iframes), "iframe(s) on the page. Data might be inside.\n")
# Switching to iframes requires specific code: remDr$switchToFrame(iframes[[1]])
# And switching back: remDr$switchToFrame(NULL)
# We'll ignore this for now unless XPaths below fail completely.
} else {
cat("DEBUG: No iframes found.\n")
}
# Find Header Element DIRECTLY using Selenium
cat("\nAttempting to find header element using Selenium with XPath:\n", xpath_header, "\n")
header_elements <- remDr$findElements(using = 'xpath', xpath_header)
if (length(header_elements) > 0) {
cat("SUCCESS: Found", length(header_elements), "header element(s) via Selenium.\n")
first_header_element <- header_elements[[1]]
header_text <- tryCatch(first_header_element$getElementText()[[1]], error = function(e) "[Error getting text]")
cat("   Text of first header found:", substr(header_text, 1, 100), "\n")
# Try parsing date from this specific header
date <- parse_week_date(header_text)
if(!is.na(date)) {
cat("   Successfully parsed date from header:", format(date), "\n")
# Find Table Element DIRECTLY using Selenium
cat("\nAttempting to find table element using Selenium with XPath:\n", xpath_table, "\n")
table_elements <- remDr$findElements(using = 'xpath', xpath_table)
if (length(table_elements) > 0) {
cat("SUCCESS: Found", length(table_elements), "table element(s) via Selenium.\n")
first_table_element <- table_elements[[1]]
table_tag_name <- tryCatch(first_table_element$getElementTagName()[[1]], error = function(e) "[Error getting tag]")
cat("   Tag name of first element found:", table_tag_name, "\n")
# If we found both header and table via Selenium, NOW try rvest parsing
cat("\nAttempting to parse page source with rvest...\n")
page_source <- remDr$getPageSource()[[1]]
page <- read_html(page_source)
# Use rvest to find the *same* nodes using the same XPaths
# This verifies rvest sees what Selenium sees
rvest_header_node <- page %>% html_node(xpath = xpath_header)
rvest_table_node <- page %>% html_node(xpath = xpath_table)
if (!is.na(xml_name(rvest_header_node))) {
cat("   DEBUG: Found header node via rvest.\n")
} else {
cat("   WARNING: Could NOT find header node via rvest, even though Selenium did.\n")
}
if (!is.na(xml_name(rvest_table_node)) && xml_name(rvest_table_node) == "table") {
cat("   DEBUG: Found table node via rvest.\n")
cat("   Passing rvest table node to extract_table_data...\n")
# Call the extraction function
table_data <- extract_table_data(rvest_table_node, date)
if (!is.null(table_data) && nrow(table_data) > 0) {
table_data$week_ending <- date
table_data$year <- year(date)
table_data$source_url <- url_to_debug # Use the debug URL
all_data[[1]] <- table_data # Store result
cat("   SUCCESS: extract_table_data returned", nrow(table_data), "rows.\n")
} else {
cat("   ERROR: extract_table_data failed or returned no rows for the rvest node.\n")
}
} else {
cat("   WARNING: Could NOT find table node via rvest, even though Selenium did.\n")
}
} else {
cat("ERROR: Could NOT find table element via Selenium using XPath:", xpath_table, "\n")
}
} else {
cat("ERROR: Could not parse date from the header text found by Selenium.\n")
}
} else {
cat("ERROR: Could NOT find header element via Selenium using XPath:", xpath_header, "\n")
}
}, finally = {
# --- 3. Stop RSelenium Driver ---
cat("\nClosing Selenium driver...\n")
tryCatch({ if (!is.null(remDr)) remDr$close() }, error = function(e) {})
tryCatch({ if (!is.null(driver$server)) driver$server$stop() }, error = function(e) {})
gc()
cat("Selenium driver closed.\n")
}) # End main tryCatch block
# --- 4. Combine and Return Data ---
if (length(all_data) > 0) {
result <- bind_rows(all_data)
} else {
result <- data.frame(jurisdiction = character(0), initial_claims = numeric(0), week_ending = as.Date(character(0)), year = integer(0), source_url = character(0), stringsAsFactors = FALSE)
}
return(result)
}
# --- Execution ---
cat("Starting DEBUGGING Maryland UI claims scraper using RSelenium\n")
start_time <- Sys.time()
# Call the new Selenium-based DEBUG function
ui_data <- debug_md_ui_claims_selenium()
# Load required libraries
if (!require(RSelenium)) install.packages("RSelenium"); library(RSelenium)
if (!require(rvest)) install.packages("rvest"); library(rvest)
if (!require(dplyr)) install.packages("dplyr"); library(dplyr)
if (!require(stringr)) install.packages("stringr"); library(stringr)
if (!require(lubridate)) install.packages("lubridate"); library(lubridate)
if (!require(xml2)) install.packages("xml2"); library(xml2)
if (!require(netstat)) install.packages("netstat"); library(netstat)
# --- Configuration ---
counties <- c("Allegany", "Anne Arundel", "Baltimore City", "Baltimore County",
"Calvert", "Caroline", "Carroll", "Cecil", "Charles", "Dorchester",
"Frederick", "Garrett", "Harford", "Howard", "Kent", "Montgomery",
"Prince George's", "Queen Anne's", "St. Mary's", "Somerset",
"Talbot", "Washington", "Wicomico", "Worcester", "Maryland")
# --- FOCUS ON FIRST URL FOR DEBUGGING ---
url_to_debug <- "https://labor.maryland.gov/employment/uicounty.shtml"
# --- Helper Functions ---
# Function to parse dates from headers
parse_week_date <- function(text) {
# Remove "Week Ending", potential colons, and trim whitespace aggressively
date_str <- gsub(".*Week [Ee]nding:?", "", text) # Remove everything up to and including "Week Ending" or "Week Ending:"
date_str <- trimws(date_str)
# Add a check for empty string after cleaning
if (nchar(date_str) == 0) {
return(NA)
}
# Try parsing with expected formats
parsed_date <- tryCatch({
mdy(date_str) # Try mdy first
}, warning = function(w) NA, error = function(e) NA)
return(parsed_date)
}
# Function to extract table data from an rvest table node
extract_table_data <- function(table_node, date) {
# cat("--- DEBUG: extract_table_data called for date", format(date), "---\n") # Optional debug
parsed_tables <- tryCatch({
html_table(table_node, fill = TRUE, header = TRUE)
}, error = function(e) {
# cat("      DEBUG extract_table_data: Error in html_table():", e$message, "\n") # Optional debug
return(NULL)
})
if (is.null(parsed_tables) || !is.list(parsed_tables) || length(parsed_tables) == 0) {
table_df <- NULL
} else {
if (!is.data.frame(parsed_tables[[1]])) {
table_df <- NULL
} else {
table_df <- parsed_tables[[1]]
}
}
if (is.null(table_df) || nrow(table_df) < 1 || ncol(table_df) < 2) {
return(NULL)
}
first_col_text <- as.character(table_df[[1]])
found_counties <- sum(sapply(counties, function(county) any(grepl(county, first_col_text, ignore.case = TRUE))))
if (found_counties < 3) { # Threshold for considering it a valid county table
return(NULL)
}
results <- data.frame(jurisdiction = character(0), initial_claims = numeric(0), stringsAsFactors = FALSE)
original_colnames <- colnames(table_df)
if(is.null(original_colnames)) original_colnames <- paste0("X", 1:ncol(table_df))
clean_colnames <- tolower(gsub("[^[:alnum:]]", "", original_colnames))
clean_colnames[clean_colnames == ""] <- paste0("empty", seq_along(clean_colnames[clean_colnames == ""]))
colnames(table_df) <- make.unique(clean_colnames)
jurisdiction_col_idx <- 1
claims_col_idx <- NULL
possible_claims_cols <- c("initialclaims", "claims", "number", "count", "total")
claims_col_match <- intersect(possible_claims_cols, colnames(table_df))
if (length(claims_col_match) > 0) {
claims_col_idx <- which(colnames(table_df) == claims_col_match[1])
} else if (ncol(table_df) >= 2) {
claims_col_idx <- 2 # Fallback to second column
}
if (is.null(claims_col_idx)) {
return(NULL) # Cannot find claims column
}
for (county in counties) {
row_matches <- which(grepl(county, table_df[[jurisdiction_col_idx]], ignore.case = TRUE))
if (length(row_matches) > 0) {
exact_match_pattern <- paste0("^\\s*", gsub("'", "['`]?s?", county), "\\s*$")
exact_row_idx <- which(grepl(exact_match_pattern, table_df[[jurisdiction_col_idx]], ignore.case = TRUE))
row_idx <- if(length(exact_row_idx) > 0) exact_row_idx[1] else row_matches[1] # Prefer exact match
claims_value_raw <- table_df[row_idx, claims_col_idx]
claims_value <- tryCatch({
cleaned_str <- gsub("[^0-9.]", "", as.character(claims_value_raw)) # Keep digits and decimal
if (nchar(cleaned_str) == 0 || cleaned_str == ".") NA_real_ else as.numeric(cleaned_str)
}, warning = function(w) NA_real_, error = function(e) NA_real_)
if (!is.na(claims_value)) {
results <- rbind(results, data.frame(jurisdiction = county, initial_claims = claims_value, stringsAsFactors = FALSE))
}
}
}
if (nrow(results) > 0) {
return(results)
} else {
return(NULL)
}
}
# --- Main RSelenium Debugging Function ---
debug_md_ui_claims_selenium <- function() {
driver <- NULL
remDr <- NULL
all_data <- list()
# --- 1. Start RSelenium Driver ---
tryCatch({
free_port <- netstat::free_port()
cat("Attempting to start Selenium driver on port", free_port, "\n")
# --- !!! EDIT THIS PATH !!! ---
# Specify the FULL path to your manually downloaded chromedriver.exe
# Example Windows: "C:/Users/YourUser/Downloads/chromedriver-win64/chromedriver.exe"
# Example Mac: "/Users/youruser/Downloads/chromedriver-mac-x64/chromedriver"
# Example Linux: "/home/youruser/Downloads/chromedriver-linux64/chromedriver"
chromedriver_path <- "C:/WebDrivers/chromedriver.exe" # <--- CHANGE THIS PATH
if (!file.exists(chromedriver_path)) {
stop("ChromeDriver executable not found at specified path: ", chromedriver_path)
}
cat("Using ChromeDriver path:", chromedriver_path, "\n")
# Start driver using the specified path
driver <- rsDriver(browser = "chrome",
port = free_port,
verbose = TRUE, # Keep verbose TRUE for debugging startup
chromever = NULL, # Important: Don't manage version automatically
driver = chromedriver_path) # Specify the path
# Check if client object exists and is valid BEFORE assigning
if (!is.null(driver) && !is.null(driver$client) && inherits(driver$client, "remoteDriver")) {
remDr <- driver$client
# Try a simple command to verify connection
cat("Attempting to get server status...\n")
status <- tryCatch(remDr$getStatus(), error = function(e) {
cat("Error getting status:", e$message, "\n"); NULL
})
if(is.null(status)) {
stop("Failed to communicate with Selenium server after startup.")
}
cat("Selenium driver and client started successfully. Server Status OK.\n")
# Set implicit wait time AFTER confirming connection
remDr$setTimeout(type = "implicit", milliseconds = 10000) # Wait up to 10s for elements
} else {
stop("Failed to create a valid Selenium client object.") # Force an error
}
}, error = function(e) {
cat("Error during Selenium startup: ", e$message, "\n")
# Ensure driver/remDr are NULL if startup failed
driver <<- NULL # Use <<- to modify variable in parent environment from function
remDr <<- NULL
})
# Explicitly check if remDr is valid before proceeding
if (is.null(remDr)) {
cat("FATAL: Selenium remote driver (remDr) is NULL. Cannot proceed.\n")
return(data.frame())
}
# --- 2. Debug Specific URL ---
tryCatch({
cat("\n----------------------------\n")
cat("Processing URL:", url_to_debug, "\n") # url_to_debug is defined outside
# Navigate
cat("Navigating to URL...\n")
remDr$navigate(url_to_debug)
Sys.sleep(3) # Allow time for page rendering after navigation command returns
cat("Navigation command sent. Current URL:", remDr$getCurrentUrl(), "\n")
cat("Page Title:", remDr$getTitle(), "\n")
# --- !!! PASTE YOUR XPATHS HERE !!! ---
# Replace these example XPaths with the ones you copied from the browser inspector
# Right-click element -> Inspect -> Right-click highlighted HTML -> Copy -> Copy XPath
xpath_header <- "/html/body/div[1]/div[3]/div/div/div[1]/article/div/p[5]/strong" #<-- REPLACE THIS
xpath_table <- "/html/body/div[1]/div[3]/div/div/div[1]/article/div/table[1]" #<-- REPLACE THIS
# --- !!! END OF XPATH SECTION !!! ---
# Check for Iframes (optional but good practice)
iframes <- remDr$findElements(using = 'tag name', 'iframe')
if (length(iframes) > 0) {
cat("WARNING: Found", length(iframes), "iframe(s) on the page. Data might be inside.\n")
# If data is inside an iframe, you need: remDr$switchToFrame(iframes[[1]]) before finding elements
# And: remDr$switchToFrame(NULL) to switch back out
} else {
cat("DEBUG: No iframes found.\n")
}
# Find Header Element DIRECTLY using Selenium
cat("\nAttempting to find header element using Selenium with XPath:\n", xpath_header, "\n")
header_elements <- remDr$findElements(using = 'xpath', xpath_header)
if (length(header_elements) > 0) {
cat("SUCCESS: Found", length(header_elements), "header element(s) via Selenium.\n")
first_header_element <- header_elements[[1]]
header_text <- tryCatch(first_header_element$getElementText()[[1]], error = function(e) "[Error getting text]")
cat("   Text of first header found:", substr(header_text, 1, 100), "\n")
# Try parsing date from this specific header
date <- parse_week_date(header_text)
if(!is.na(date)) {
cat("   Successfully parsed date from header:", format(date), "\n")
# Find Table Element DIRECTLY using Selenium
cat("\nAttempting to find table element using Selenium with XPath:\n", xpath_table, "\n")
table_elements <- remDr$findElements(using = 'xpath', xpath_table)
if (length(table_elements) > 0) {
cat("SUCCESS: Found", length(table_elements), "table element(s) via Selenium.\n")
first_table_element <- table_elements[[1]]
table_tag_name <- tryCatch(first_table_element$getElementTagName()[[1]], error = function(e) "[Error getting tag]")
cat("   Tag name of first element found:", table_tag_name, "\n")
# If we found both header and table via Selenium, NOW try rvest parsing
cat("\nAttempting to parse page source with rvest...\n")
page_source <- remDr$getPageSource()[[1]]
page <- read_html(page_source)
# Use rvest to find the *same* nodes using the same XPaths
# This verifies rvest sees what Selenium sees
rvest_header_node <- page %>% html_node(xpath = xpath_header)
rvest_table_node <- page %>% html_node(xpath = xpath_table)
if (!is.na(xml_name(rvest_header_node))) {
cat("   DEBUG: Found header node via rvest.\n")
} else {
cat("   WARNING: Could NOT find header node via rvest, even though Selenium did.\n")
}
if (!is.na(xml_name(rvest_table_node)) && xml_name(rvest_table_node) == "table") {
cat("   DEBUG: Found table node via rvest.\n")
cat("   Passing rvest table node to extract_table_data...\n")
# Call the extraction function
table_data <- extract_table_data(rvest_table_node, date)
if (!is.null(table_data) && nrow(table_data) > 0) {
table_data$week_ending <- date
table_data$year <- year(date)
table_data$source_url <- url_to_debug # Use the debug URL
all_data[[1]] <- table_data # Store result
cat("   SUCCESS: extract_table_data returned", nrow(table_data), "rows.\n")
} else {
cat("   ERROR: extract_table_data failed or returned no rows for the rvest node.\n")
}
} else {
cat("   WARNING: Could NOT find table node via rvest, even though Selenium did.\n")
}
} else {
cat("ERROR: Could NOT find table element via Selenium using XPath:", xpath_table, "\n")
}
} else {
cat("ERROR: Could not parse date from the header text found by Selenium.\n")
}
} else {
cat("ERROR: Could NOT find header element via Selenium using XPath:", xpath_header, "\n")
}
}, finally = {
# --- 3. Stop RSelenium Driver ---
cat("\nClosing Selenium driver...\n")
tryCatch({ if (!is.null(remDr)) remDr$close() }, error = function(e) {cat("Note: Error closing client (often ignorable):", e$message, "\n")})
tryCatch({ if (!is.null(driver$server)) driver$server$stop() }, error = function(e) {cat("Note: Error stopping server (often ignorable):", e$message, "\n")})
# Kill potential zombie processes
if (.Platform$OS.type == "windows") {
system("taskkill /im chromedriver.exe /f", ignore.stdout = TRUE, ignore.stderr = TRUE)
system("taskkill /im java.exe /f", ignore.stdout = TRUE, ignore.stderr = TRUE) # If Selenium Server JAR was used
} else {
# Add equivalent kill commands for Mac/Linux if needed (e.g., pkill chromedriver)
}
gc() # Garbage collection
cat("Selenium driver closed.\n")
}) # End main tryCatch block
# --- 4. Combine and Return Data ---
if (length(all_data) > 0) {
result <- bind_rows(all_data)
} else {
# Return empty frame with correct structure
result <- data.frame(jurisdiction = character(0), initial_claims = numeric(0), week_ending = as.Date(character(0)), year = integer(0), source_url = character(0), stringsAsFactors = FALSE)
}
return(result)
}
# --- Execution ---
cat("Starting DEBUGGING Maryland UI claims scraper using RSelenium\n")
start_time <- Sys.time()
# Call the new Selenium-based DEBUG function
ui_data <- debug_md_ui_claims_selenium()
end_time <- Sys.time()
elapsed <- end_time - start_time
cat("\n----------------------------\n")
cat("DEBUGGING Scraping completed in", round(elapsed, 2), "seconds\n")
# --- Post-processing and Output (only if data was found) ---
cat("\nResults summary:\n")
cat("Total rows extracted during debug:", nrow(ui_data), "\n")
if (nrow(ui_data) > 0) {
cat("Data found for week ending:", format(ui_data$week_ending[1]), "\n")
print(head(ui_data))
# Optional: Save the debug output
# output_file_debug <- "maryland_ui_claims_debug_output.csv"
# tryCatch({
#     write.csv(ui_data, output_file_debug, row.names = FALSE, na = "")
#     cat("\nSaved debug results to", output_file_debug, "\n")
# }, error = function(e){
#     cat("\nError saving debug CSV:", e$message, "\n")
# })
} else {
cat("No data was successfully extracted during the debug run.\n")
}
cat("\nScript execution complete.\n")
view(dw_basemaps)
library(DatawRappr)
library(tidyverse)
datawrapper_auth(api_key =  "YDefGtmJ90tDbh1TQXGUAtuATxk1nCAt5dy3AW2dRnx4orG9NxUV6IDjrlJv3nLD", overwrite=TRUE)
library(janitor)
library(lubridate)
library(htmltools)
view(dw_basemaps)
wsp_data <- read_csv("worker_support_data.csv") |> clean_names()
wsp_data$zip <- as.character(wsp_data$zip)
View(wsp_data)
wsp_data <- wsp_data |> rename(zip = zip_code)
wsp_data$zip <- as.character(wsp_data$zip)
wsp_data <- read_csv("worker_support_data.csv") |> clean_names()
wsp_data <- wsp_data |> rename(zip = zip_code)
wsp_data$zip <- as.character(wsp_data$zip)
zip_database <- read_csv("zip_code_database.csv")
wsp_joined <- left_join(wsp_data, zip_database, by="zip")
wsp_joined$zip <- as.character(wsp_joined$zip)
dw_data_to_chart(wsp_joined, wsp_chart)
south_wsp_chart <- dw_create_chart()
dw_data_to_chart(wsp_joined, south_wsp_chart)
dw_edit_chart(south_wsp_chart, type="south-zip")
library(DatawRappr)
library(tidyverse)
datawrapper_auth(api_key =  "YDefGtmJ90tDbh1TQXGUAtuATxk1nCAt5dy3AW2dRnx4orG9NxUV6IDjrlJv3nLD", overwrite=TRUE)
library(janitor)
library(lubridate)
library(htmltools)
dw_edit_chart(south_wsp_chart, type="south-zip")
library(DatawRappr)
library(tidyverse)
datawrapper_auth(api_key =  "vWqY9z6ikqSKVEk8PI9RLQ8wpspkmGKTXLBFbKZ5V8QVrGo5bk2Sxd7DlDsKY9D3", overwrite=TRUE)
library(janitor)
library(lubridate)
library(htmltools)
dw_edit_chart(south_wsp_chart, type="south-zip")
dw_edit_chart(south_wsp_chart, type="d3-maps-choropleth", visualize = list(
basemap="south-zip"
), )
wsp_joined <- left_join(wsp_data, zip_database, by="zip")
wsp_joined$zip <- as.character(wsp_joined$zip)
dw_data_to_chart(wsp_joined, south_wsp_chart)
dw_edit_chart(south_wsp_chart, type="d3-maps-choropleth", visualize = list(
basemap="south-zip"
), )
write.csv(wsp_joined, "wsp_joined.csv", row.names = FALSE)
wsp_recipients_by_state <- wsp_joined |>
group_by(state) |>
summarize(total_recipients = sum(number_of_claimants)) |>
arrange(desc(total_recipients))
library(DatawRappr)
library(tidyverse)
datawrapper_auth(api_key =  "vWqY9z6ikqSKVEk8PI9RLQ8wpspkmGKTXLBFbKZ5V8QVrGo5bk2Sxd7DlDsKY9D3", overwrite=TRUE)
library(janitor)
library(lubridate)
library(htmltools)
wsp_recipients_by_state <- wsp_joined |>
group_by(state) |>
summarize(total_recipients = sum(number_of_claimants)) |>
arrange(desc(total_recipients))
wsp_zips_by_state <- wsp_joined |>
group_by(state) |>
summarize(count_of_zip = n()) |>
arrange(desc(zip_codes_with_claimant))
wsp_zips_by_state <- wsp_joined |>
group_by(state) |>
summarize(zip_codes_with_claimant = n()) |>
arrange(desc(zip_codes_with_claimant))
View(wsp_recipients_by_state)
View(wsp_zips_by_state)
wsp_summary_combined <- wsp_recipients_by_state |> left_join(wsp_zips_by_state, join_by=state)
wsp_summary_combined <- wsp_recipients_by_state |> left_join(wsp_zips_by_state, join_by="state")
wsp_summary_combined <- left_join(wsp_zips_by_state, wsp_recipients_by_state, by="state")
View(wsp_summary_combined)
bullet_chart <- dw_create_chart()
dw_data_to_chart(wsp_summary_combined, bullet_chart)
dw_edit_chart(type="d3-bars-bullet")
dw_edit_chart(bullet_chart, type="d3-bars-bullet")
dw_retrieve_chart_metadata(bullet_chart)
dw_edit_chart(bullet_chart, type="d3-bars-bullet", byline="Matt Cohen", source_name = "Maryland Department of Labor")
dw_edit_chart(bullet_chart, type="d3-bars-bullet", byline="Matt Cohen", source_name = "Maryland Department of Labor", title="WSP claims by state, zip codes per state")
dw_publish_chart(bullet_chart)
write.csv(wsp_summary_combined, "wsp_summary.csv", row.names=FALSE)
View(wsp_joined)

---
title: "UI Scraper"
author: "Wells"
date: "2025-04-07"
output: html_document
---

```{r}
# Load required libraries
library(rvest)
library(dplyr)
library(stringr)
library(lubridate)
library(xml2)

# URL of the website to scrape
url <- "https://labor.maryland.gov/employment/uicounty.shtml"

# Function to parse date
parse_date <- function(text) {
  # Extract date part after "Week Ending"
  date_str <- gsub("Week Ending", "", text)
  date_str <- trimws(date_str)
  
  # Try to parse the date
  tryCatch({
    as.Date(mdy(date_str))
  }, error = function(e) {
    NA
  })
}

# Main scraping function
scrape_md_ui_claims <- function() {
  # Create a list to store data frames for each week
  all_weeks <- list()
  
  # Try to fetch the webpage
  tryCatch({
    cat("Fetching webpage...\n")
    page <- read_html(url)
    cat("Successfully fetched webpage\n")
    
    # Save raw HTML for inspection
    writeLines(as.character(page), "raw_page.html")
    cat("Saved raw HTML to raw_page.html\n")
    
    # Extract all week headers with dates
    week_headers <- page %>% 
      html_nodes("strong") %>%
      html_text() %>%
      .[grepl("Week Ending", .)]
    
    cat("Found", length(week_headers), "week headers\n")
    
    # For each week header, try to extract the associated table using HTML patterns
    for (i in 1:length(week_headers)) {
      week_text <- week_headers[i]
      week_date <- parse_date(week_text)
      
      if (!is.na(week_date) && year(week_date) == 2025) {
        cat("Processing week:", format(week_date, "%B %d, %Y"), "\n")
        
        # Find this text in the page source
        html_text <- as.character(page)
        
        # Look for the section heading with county data
        county_heading <- paste0("MARYLAND DEPARTMENT OF LABOR DIVISION OF UNEMPLOYMENT INSURANCE TOTAL CLAIMS FILED BY COUNTY Week ending ", 
                                format(week_date, "%B %d, %Y"))
        
        # This pattern appears in the debugging output
        county_pos <- str_locate(html_text, fixed(county_heading))
        
        if (!is.na(county_pos[1,1])) {
          cat("Found county data section\n")
          
          # Extract a chunk of HTML after this heading
          start_pos <- county_pos[1,2]
          end_pos <- min(start_pos + 10000, nchar(html_text))
          chunk <- substr(html_text, start_pos, end_pos)
          
          # Let's look for patterns that indicate the data we want
          # Based on the debug output, we know there are sections for:
          # Regular UI, PUA NEW, PUA (Reclassified), PEUC Claims
          
          # For this approach, let's focus on Regular UI claims
          reg_ui_pos <- str_locate(chunk, "Regular UI")
          
          if (!is.na(reg_ui_pos[1,1])) {
            cat("Found Regular UI section\n")
            
            # Extract chunk after Regular UI heading
            ui_chunk_start <- reg_ui_pos[1,2]
            ui_chunk_end <- str_locate(chunk, "PUA NEW")[1,1]
            
            if (is.na(ui_chunk_end)) {
              ui_chunk_end <- min(ui_chunk_start + 5000, nchar(chunk))
            }
            
            ui_chunk <- substr(chunk, ui_chunk_start, ui_chunk_end)
            
            # Extract data from this chunk manually
            # Assuming the data is in a table-like format with counties and numbers
            # We'll use regex patterns to extract this data
            
            # Define Maryland counties
            counties <- c("Allegany", "Anne Arundel", "Baltimore City", "Baltimore County", 
                          "Calvert", "Caroline", "Carroll", "Cecil", "Charles", "Dorchester", 
                          "Frederick", "Garrett", "Harford", "Howard", "Kent", "Montgomery", 
                          "Prince George's", "Queen Anne's", "St. Mary's", "Somerset", 
                          "Talbot", "Washington", "Wicomico", "Worcester", "Maryland")
            
            # Create empty data frame for this week
            week_data <- data.frame(
              jurisdiction = character(0),
              initial_claims = numeric(0),
              stringsAsFactors = FALSE
            )
            
            # Try to extract data for each county
            for (county in counties) {
              # Look for the county name followed by numbers
              pattern <- paste0(county, "\\s*([0-9,]+)")
              matches <- str_match(ui_chunk, pattern)
              
              if (!is.na(matches[1,2])) {
                # Extract the claims value
                claims <- as.numeric(gsub(",", "", matches[1,2]))
                
                # Add to data frame
                week_data <- rbind(week_data, data.frame(
                  jurisdiction = county,
                  initial_claims = claims,
                  stringsAsFactors = FALSE
                ))
                
                cat("  Found data for", county, ":", claims, "\n")
              }
            }
            
            # Add the week ending date
            if (nrow(week_data) > 0) {
              week_data$week_ending <- week_date
              all_weeks[[i]] <- week_data
              cat("Added data for week to results\n")
            }
          }
        }
      }
    }
    
  }, error = function(e) {
    cat("Error processing page:", e$message, "\n")
  })
  
  # Combine all weeks into a single data frame
  result <- do.call(rbind, all_weeks)
  
  # Ensure we have a data frame even if empty
  if (is.null(result) || nrow(result) == 0) {
    result <- data.frame(
      jurisdiction = character(0),
      initial_claims = numeric(0),
      week_ending = as.Date(character(0)),
      stringsAsFactors = FALSE
    )
  }
  
  return(result)
}

# Run the scraper
cat("Starting Maryland UI claims scraper\n")
ui_data <- scrape_md_ui_claims()

# Show results
cat("\nResults summary:\n")
cat("Total rows:", nrow(ui_data), "\n")
cat("Unique weeks:", length(unique(ui_data$week_ending)), "\n")

if (nrow(ui_data) > 0) {
  # Reorder columns
  ui_data <- ui_data %>%
    select(week_ending, jurisdiction, initial_claims)
  
  # Display sample
  cat("Sample data:\n")
  print(head(ui_data))
}

# Always save to CSV
output_file <- "maryland_ui_claims_2025.csv"
write.csv(ui_data, output_file, row.names = FALSE)
cat("Saved results to", output_file, "\n")

cat("Script execution complete.\n")
```


#Basic scraper for one article

```{r}
library(rvest)

# Define the URL
url2 <- "https://www.dailywire.com/news/not-encouraging-nyt-warns-kamala-in-trouble-as-poll-finds-her-tied-with-trump?topStoryPosition=1"

# Read the HTML content of the page
page <- read_html(url2)

# Extract the headline (usually in an <h1> tag)
headline <- page %>%
  html_element("h1") %>%
  html_text(trim = TRUE)

# Extract the article text (all <p> elements)
article_text <- page %>%
  html_elements("p") %>%
  html_text(trim = TRUE)

# Print the headline and article content
cat("Headline:\n", headline, "\n\n")
cat("Article Text:\n", paste(article_text, collapse = "\n"))

```

```{r}
# Load required libraries
if (!require(RSelenium)) install.packages("RSelenium"); library(RSelenium)
if (!require(rvest)) install.packages("rvest"); library(rvest)
if (!require(dplyr)) install.packages("dplyr"); library(dplyr)
if (!require(stringr)) install.packages("stringr"); library(stringr)
if (!require(lubridate)) install.packages("lubridate"); library(lubridate)
if (!require(xml2)) install.packages("xml2"); library(xml2)
if (!require(netstat)) install.packages("netstat"); library(netstat)

# --- Configuration ---
counties <- c("Allegany", "Anne Arundel", "Baltimore City", "Baltimore County",
              "Calvert", "Caroline", "Carroll", "Cecil", "Charles", "Dorchester",
              "Frederick", "Garrett", "Harford", "Howard", "Kent", "Montgomery",
              "Prince George's", "Queen Anne's", "St. Mary's", "Somerset",
              "Talbot", "Washington", "Wicomico", "Worcester", "Maryland")

# --- FOCUS ON FIRST URL FOR DEBUGGING ---
url_to_debug <- "https://labor.maryland.gov/employment/uicounty.shtml"

# --- Helper Functions ---

# Function to parse dates from headers
parse_week_date <- function(text) {
  # Remove "Week Ending", potential colons, and trim whitespace aggressively
  date_str <- gsub(".*Week [Ee]nding:?", "", text) # Remove everything up to and including "Week Ending" or "Week Ending:"
  date_str <- trimws(date_str)

  # Add a check for empty string after cleaning
  if (nchar(date_str) == 0) {
      return(NA)
  }

  # Try parsing with expected formats
  parsed_date <- tryCatch({
      mdy(date_str) # Try mdy first
  }, warning = function(w) NA, error = function(e) NA)

  return(parsed_date)
}

# Function to extract table data from an rvest table node
extract_table_data <- function(table_node, date) {
  # cat("--- DEBUG: extract_table_data called for date", format(date), "---\n") # Optional debug

  parsed_tables <- tryCatch({
    html_table(table_node, fill = TRUE, header = TRUE)
  }, error = function(e) {
    # cat("      DEBUG extract_table_data: Error in html_table():", e$message, "\n") # Optional debug
    return(NULL)
  })

  if (is.null(parsed_tables) || !is.list(parsed_tables) || length(parsed_tables) == 0) {
     table_df <- NULL
  } else {
     if (!is.data.frame(parsed_tables[[1]])) {
        table_df <- NULL
     } else {
        table_df <- parsed_tables[[1]]
     }
  }

  if (is.null(table_df) || nrow(table_df) < 1 || ncol(table_df) < 2) {
    return(NULL)
  }

  first_col_text <- as.character(table_df[[1]])
  found_counties <- sum(sapply(counties, function(county) any(grepl(county, first_col_text, ignore.case = TRUE))))

  if (found_counties < 3) { # Threshold for considering it a valid county table
     return(NULL)
  }

  results <- data.frame(jurisdiction = character(0), initial_claims = numeric(0), stringsAsFactors = FALSE)
  original_colnames <- colnames(table_df)
  if(is.null(original_colnames)) original_colnames <- paste0("X", 1:ncol(table_df))
  clean_colnames <- tolower(gsub("[^[:alnum:]]", "", original_colnames))
  clean_colnames[clean_colnames == ""] <- paste0("empty", seq_along(clean_colnames[clean_colnames == ""]))
  colnames(table_df) <- make.unique(clean_colnames)

  jurisdiction_col_idx <- 1
  claims_col_idx <- NULL
  possible_claims_cols <- c("initialclaims", "claims", "number", "count", "total")
  claims_col_match <- intersect(possible_claims_cols, colnames(table_df))

  if (length(claims_col_match) > 0) {
      claims_col_idx <- which(colnames(table_df) == claims_col_match[1])
  } else if (ncol(table_df) >= 2) {
      claims_col_idx <- 2 # Fallback to second column
  }

  if (is.null(claims_col_idx)) {
      return(NULL) # Cannot find claims column
  }

  for (county in counties) {
    row_matches <- which(grepl(county, table_df[[jurisdiction_col_idx]], ignore.case = TRUE))
    if (length(row_matches) > 0) {
      exact_match_pattern <- paste0("^\\s*", gsub("'", "['`]?s?", county), "\\s*$")
      exact_row_idx <- which(grepl(exact_match_pattern, table_df[[jurisdiction_col_idx]], ignore.case = TRUE))
      row_idx <- if(length(exact_row_idx) > 0) exact_row_idx[1] else row_matches[1] # Prefer exact match

      claims_value_raw <- table_df[row_idx, claims_col_idx]
      claims_value <- tryCatch({
          cleaned_str <- gsub("[^0-9.]", "", as.character(claims_value_raw)) # Keep digits and decimal
          if (nchar(cleaned_str) == 0 || cleaned_str == ".") NA_real_ else as.numeric(cleaned_str)
      }, warning = function(w) NA_real_, error = function(e) NA_real_)

      if (!is.na(claims_value)) {
          results <- rbind(results, data.frame(jurisdiction = county, initial_claims = claims_value, stringsAsFactors = FALSE))
      }
    }
  }

  if (nrow(results) > 0) {
    return(results)
  } else {
    return(NULL)
  }
}


# --- Main RSelenium Debugging Function ---
debug_md_ui_claims_selenium <- function() {

  driver <- NULL
  remDr <- NULL
  all_data <- list()

  # --- 1. Start RSelenium Driver ---
  tryCatch({
    free_port <- netstat::free_port()
    cat("Attempting to start Selenium driver on port", free_port, "\n")

    # --- !!! EDIT THIS PATH !!! ---
    # Specify the FULL path to your manually downloaded chromedriver.exe
    # Example Windows: "C:/Users/YourUser/Downloads/chromedriver-win64/chromedriver.exe"
    # Example Mac: "/Users/youruser/Downloads/chromedriver-mac-x64/chromedriver"
    # Example Linux: "/home/youruser/Downloads/chromedriver-linux64/chromedriver"
    chromedriver_path <- "C:/WebDrivers/chromedriver.exe" # <--- CHANGE THIS PATH

    if (!file.exists(chromedriver_path)) {
        stop("ChromeDriver executable not found at specified path: ", chromedriver_path)
    }
    cat("Using ChromeDriver path:", chromedriver_path, "\n")

    # Start driver using the specified path
    driver <- rsDriver(browser = "chrome",
                       port = free_port,
                       verbose = TRUE, # Keep verbose TRUE for debugging startup
                       chromever = NULL, # Important: Don't manage version automatically
                       driver = chromedriver_path) # Specify the path

    # Check if client object exists and is valid BEFORE assigning
    if (!is.null(driver) && !is.null(driver$client) && inherits(driver$client, "remoteDriver")) {
        remDr <- driver$client
        # Try a simple command to verify connection
        cat("Attempting to get server status...\n")
        status <- tryCatch(remDr$getStatus(), error = function(e) {
             cat("Error getting status:", e$message, "\n"); NULL
             })
        if(is.null(status)) {
            stop("Failed to communicate with Selenium server after startup.")
        }
        cat("Selenium driver and client started successfully. Server Status OK.\n")
        # Set implicit wait time AFTER confirming connection
        remDr$setTimeout(type = "implicit", milliseconds = 10000) # Wait up to 10s for elements
    } else {
        stop("Failed to create a valid Selenium client object.") # Force an error
    }

  }, error = function(e) {
    cat("Error during Selenium startup: ", e$message, "\n")
    # Ensure driver/remDr are NULL if startup failed
    driver <<- NULL # Use <<- to modify variable in parent environment from function
    remDr <<- NULL
  })

  # Explicitly check if remDr is valid before proceeding
  if (is.null(remDr)) {
    cat("FATAL: Selenium remote driver (remDr) is NULL. Cannot proceed.\n")
    return(data.frame())
  }

  # --- 2. Debug Specific URL ---
  tryCatch({
    cat("\n----------------------------\n")
    cat("Processing URL:", url_to_debug, "\n") # url_to_debug is defined outside

    # Navigate
    cat("Navigating to URL...\n")
    remDr$navigate(url_to_debug)
    Sys.sleep(3) # Allow time for page rendering after navigation command returns
    cat("Navigation command sent. Current URL:", remDr$getCurrentUrl(), "\n")
    cat("Page Title:", remDr$getTitle(), "\n")

    # --- !!! PASTE YOUR XPATHS HERE !!! ---
    # Replace these example XPaths with the ones you copied from the browser inspector
    # Right-click element -> Inspect -> Right-click highlighted HTML -> Copy -> Copy XPath
    xpath_header <- "/html/body/div[1]/div[3]/div/div/div[1]/article/div/p[5]/strong" #<-- REPLACE THIS
    xpath_table <- "/html/body/div[1]/div[3]/div/div/div[1]/article/div/table[1]" #<-- REPLACE THIS
    # --- !!! END OF XPATH SECTION !!! ---


    # Check for Iframes (optional but good practice)
    iframes <- remDr$findElements(using = 'tag name', 'iframe')
    if (length(iframes) > 0) {
        cat("WARNING: Found", length(iframes), "iframe(s) on the page. Data might be inside.\n")
        # If data is inside an iframe, you need: remDr$switchToFrame(iframes[[1]]) before finding elements
        # And: remDr$switchToFrame(NULL) to switch back out
    } else {
        cat("DEBUG: No iframes found.\n")
    }


    # Find Header Element DIRECTLY using Selenium
    cat("\nAttempting to find header element using Selenium with XPath:\n", xpath_header, "\n")
    header_elements <- remDr$findElements(using = 'xpath', xpath_header)

    if (length(header_elements) > 0) {
        cat("SUCCESS: Found", length(header_elements), "header element(s) via Selenium.\n")
        first_header_element <- header_elements[[1]]
        header_text <- tryCatch(first_header_element$getElementText()[[1]], error = function(e) "[Error getting text]")
        cat("   Text of first header found:", substr(header_text, 1, 100), "\n")

        # Try parsing date from this specific header
        date <- parse_week_date(header_text)
        if(!is.na(date)) {
            cat("   Successfully parsed date from header:", format(date), "\n")

            # Find Table Element DIRECTLY using Selenium
            cat("\nAttempting to find table element using Selenium with XPath:\n", xpath_table, "\n")
            table_elements <- remDr$findElements(using = 'xpath', xpath_table)

            if (length(table_elements) > 0) {
                cat("SUCCESS: Found", length(table_elements), "table element(s) via Selenium.\n")
                first_table_element <- table_elements[[1]]
                table_tag_name <- tryCatch(first_table_element$getElementTagName()[[1]], error = function(e) "[Error getting tag]")
                cat("   Tag name of first element found:", table_tag_name, "\n")

                # If we found both header and table via Selenium, NOW try rvest parsing
                cat("\nAttempting to parse page source with rvest...\n")
                page_source <- remDr$getPageSource()[[1]]
                page <- read_html(page_source)

                # Use rvest to find the *same* nodes using the same XPaths
                # This verifies rvest sees what Selenium sees
                rvest_header_node <- page %>% html_node(xpath = xpath_header)
                rvest_table_node <- page %>% html_node(xpath = xpath_table)

                if (!is.na(xml_name(rvest_header_node))) {
                     cat("   DEBUG: Found header node via rvest.\n")
                } else {
                     cat("   WARNING: Could NOT find header node via rvest, even though Selenium did.\n")
                }

                if (!is.na(xml_name(rvest_table_node)) && xml_name(rvest_table_node) == "table") {
                    cat("   DEBUG: Found table node via rvest.\n")
                    cat("   Passing rvest table node to extract_table_data...\n")
                    # Call the extraction function
                    table_data <- extract_table_data(rvest_table_node, date)

                    if (!is.null(table_data) && nrow(table_data) > 0) {
                        table_data$week_ending <- date
                        table_data$year <- year(date)
                        table_data$source_url <- url_to_debug # Use the debug URL
                        all_data[[1]] <- table_data # Store result
                        cat("   SUCCESS: extract_table_data returned", nrow(table_data), "rows.\n")
                    } else {
                        cat("   ERROR: extract_table_data failed or returned no rows for the rvest node.\n")
                    }
                } else {
                    cat("   WARNING: Could NOT find table node via rvest, even though Selenium did.\n")
                }

            } else {
                cat("ERROR: Could NOT find table element via Selenium using XPath:", xpath_table, "\n")
            }
        } else {
             cat("ERROR: Could not parse date from the header text found by Selenium.\n")
        }
    } else {
        cat("ERROR: Could NOT find header element via Selenium using XPath:", xpath_header, "\n")
    }

  }, finally = {
    # --- 3. Stop RSelenium Driver ---
    cat("\nClosing Selenium driver...\n")
    tryCatch({ if (!is.null(remDr)) remDr$close() }, error = function(e) {cat("Note: Error closing client (often ignorable):", e$message, "\n")})
    tryCatch({ if (!is.null(driver$server)) driver$server$stop() }, error = function(e) {cat("Note: Error stopping server (often ignorable):", e$message, "\n")})
    # Kill potential zombie processes
    if (.Platform$OS.type == "windows") {
        system("taskkill /im chromedriver.exe /f", ignore.stdout = TRUE, ignore.stderr = TRUE)
        system("taskkill /im java.exe /f", ignore.stdout = TRUE, ignore.stderr = TRUE) # If Selenium Server JAR was used
    } else {
        # Add equivalent kill commands for Mac/Linux if needed (e.g., pkill chromedriver)
    }
    gc() # Garbage collection
    cat("Selenium driver closed.\n")
  }) # End main tryCatch block

  # --- 4. Combine and Return Data ---
  if (length(all_data) > 0) {
      result <- bind_rows(all_data)
  } else {
      # Return empty frame with correct structure
      result <- data.frame(jurisdiction = character(0), initial_claims = numeric(0), week_ending = as.Date(character(0)), year = integer(0), source_url = character(0), stringsAsFactors = FALSE)
  }
  return(result)
}

# --- Execution ---
cat("Starting DEBUGGING Maryland UI claims scraper using RSelenium\n")
start_time <- Sys.time()

# Call the new Selenium-based DEBUG function
ui_data <- debug_md_ui_claims_selenium()

end_time <- Sys.time()
elapsed <- end_time - start_time
cat("\n----------------------------\n")
cat("DEBUGGING Scraping completed in", round(elapsed, 2), "seconds\n")

# --- Post-processing and Output (only if data was found) ---
cat("\nResults summary:\n")
cat("Total rows extracted during debug:", nrow(ui_data), "\n")

if (nrow(ui_data) > 0) {
  cat("Data found for week ending:", format(ui_data$week_ending[1]), "\n")
  print(head(ui_data))

  # Optional: Save the debug output
  # output_file_debug <- "maryland_ui_claims_debug_output.csv"
  # tryCatch({
  #     write.csv(ui_data, output_file_debug, row.names = FALSE, na = "")
  #     cat("\nSaved debug results to", output_file_debug, "\n")
  # }, error = function(e){
  #     cat("\nError saving debug CSV:", e$message, "\n")
  # })

} else {
  cat("No data was successfully extracted during the debug run.\n")
}

cat("\nScript execution complete.\n")
```





